{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b501aa1b-846e-4f04-a5e2-77912d444f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoreload \n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "# Lightning #\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CometLogger\n",
    "from lightning.pytorch.utilities.rank_zero import rank_zero_only # NEW\n",
    "from datetime import datetime\n",
    "\n",
    "# Bacis libraries #\n",
    "import os   \n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('/home/pk21271/keys/key.env')\n",
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "\n",
    "# Pytorch #\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Personal scripts #\n",
    "path_src = '../src'\n",
    "if path_src not in sys.path:\n",
    "    sys.path.insert(0,path_src)\n",
    "from preprocessing import *\n",
    "from callbacks import *\n",
    "from transformer import AnalysisObjectTransformer, Embedding\n",
    "from losses import BCEDecorrelatedLoss\n",
    "from plotting import plot_roc, plot_confusion_matrix\n",
    "\n",
    "\n",
    "torch.set_float32_matmul_precision('medium')\n",
    "accelerator = 'gpu' if torch.cuda.is_available() else \"cpu\"\n",
    "print (f\"Accelerator : {accelerator}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56b48bc-eb12-4611-8579-c3953ecf1f50",
   "metadata": {},
   "source": [
    "# Import datasets\n",
    "\n",
    "Our files are in parquet files : one for each process, containing all the event level information\n",
    "\n",
    "The following lines do the import into a pandas dataframe, and the different preprocessing steps we need (remove negative weight events, make labls, split into training and validation sets, etc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaec7c5c-89df-4ef2-84bb-8c7bdd4ba78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make output directory #\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "outdir = f\"./model_training/AOTransformer_{current_time}\"\n",
    "os.makedirs(outdir, exist_ok=False)\n",
    "\n",
    "# ## Specify dataset files to run over ##\n",
    "# path = \"/cephfs/dice/projects/CMS/Hinv/ml_datasets_ul/UL{year}_ml_inputs/{dataset}.parquet\"\n",
    "\n",
    "# datasets = [\n",
    "#     'ttH_HToInvisible_M125',\n",
    "#     'TTToSemiLeptonic',\n",
    "# ]\n",
    "# years = ['2018']\n",
    "\n",
    "# files = [\n",
    "#     path.format(year=year, dataset=dataset)\n",
    "#     for dataset in datasets\n",
    "#     for year in years\n",
    "# ]\n",
    "\n",
    "# ## Data preprocessing ##\n",
    "# df = load_from_parquet(files)\n",
    "# df = remove_negative_events(df)\n",
    "# df[\"target\"] = create_target_labels(df[\"dataset\"])\n",
    "# apply_reweighting_per_class(df)\n",
    "# reweighting = torch.Tensor(df['weight_training'].values)\n",
    "# weight_nominal = torch.Tensor(df['weight_nominal'].values) # Add this to track nominal weights \n",
    "# df[\"target\"] = create_target_labels(df[\"dataset\"])\n",
    "\n",
    "# X, y, pad_mask = awkward_to_inputs_parallel(df, n_processes=8, target_length=10)\n",
    "\n",
    "# event_level = get_event_level(df)\n",
    "#split_masks = kfold_split(df, k=2)      # returns a list with a mask for each fold, so if only training one select it in line below\n",
    "#split = split_masks[0]\n",
    "\n",
    "#### IMPORT DATA ALREADY PROCESSED\n",
    "\n",
    "import torch\n",
    "\n",
    "# Main path\n",
    "main_path = '/home/pk21271/msciproject/prep_data/ttH_ttSL/'\n",
    "\n",
    "# File paths\n",
    "x_path = os.path.join(main_path, 'X.pt')\n",
    "y_path = os.path.join(main_path, 'y.pt')\n",
    "pad_mask_path = os.path.join(main_path, 'pad_mask.pt')\n",
    "reweighting_path = os.path.join(main_path, 'reweighting.pt')\n",
    "weight_nom_path = os.path.join(main_path, 'weight_nom.pt')\n",
    "event_level_path = os.path.join(main_path, 'event_level.pt')\n",
    "\n",
    "# Load data\n",
    "X = torch.load(x_path)\n",
    "y = torch.load(y_path)\n",
    "pad_mask = torch.load(pad_mask_path)\n",
    "reweighting = torch.load(reweighting_path)\n",
    "weight_nom = torch.load(weight_nom_path)\n",
    "event_level = torch.load(event_level_path)\n",
    "\n",
    "## Create training datasets ## Also added the nominal weights\n",
    "train_X, val_X, train_y, val_y, train_weights, val_weights, train_mask, val_mask, train_event, val_event, train_nom, val_nom = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    reweighting, \n",
    "    pad_mask, \n",
    "    event_level,\n",
    "    weight_nom, \n",
    "    test_size=0.2,  \n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "train_dataset = TensorDataset(train_X, train_y, train_weights, train_mask, train_event)\n",
    "valid_dataset = TensorDataset(val_X, val_y, val_weights, val_mask, val_event)\n",
    "\n",
    "## Create loaders ##\n",
    "batch_size = 2048\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    dataset = train_dataset, \n",
    "    batch_size = batch_size, \n",
    "    shuffle = True,\n",
    "    num_workers = 11, \n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    dataset = valid_dataset, \n",
    "    batch_size = 10000, # can use larger batches for the GPU \n",
    "    shuffle = False, \n",
    "    num_workers = 11,\n",
    ")\n",
    "# NOTE : in a jupyter notebook it is not possible to use `num_workers` dues to multiprocessing issues.\n",
    "# Use this notebook for testing and experimenting, but to benefit from multiprocessing batch loading, you will need the python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37519870-6bb9-40d5-a5bc-135284411f45",
   "metadata": {},
   "source": [
    "# Input plots \n",
    "\n",
    "Always a good idea to check the inputs you provide in the training. It is very easy to introduce incorrect data in the training, and then spend hours investigating the problem in the model, only to find the data is nonsensical (the good old \"garbage in, garbage out\" syndrom).\n",
    "\n",
    "The plotting scripts below show the inputs features (ie, jet features) as a function of the multiplicity and labels.\n",
    "- var 0 : $p_T$\n",
    "- var 1 : $\\eta$\n",
    "- var 2 : $\\phi$\n",
    "- var 3 : mass\n",
    "- var 4 : area\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480c909d-3869-43d3-82da-95e7c0af681b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_inputs_per_multiplicity(X,y,pad_mask,bins=100,log=True,show=True)\n",
    "fig = plot_inputs_per_label(X,y,pad_mask,bins=100,log=True,show=True)\n",
    "fig = plot_inputs_per_label(X,y,pad_mask,bins=100,weights=reweighting,log=True,show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a77034-fa39-4948-a1c2-ab90f3b2f6a5",
   "metadata": {},
   "source": [
    "# Model\n",
    "The transformer model can be tweaked with the parameters below (see explanations for each parameter.\n",
    "\n",
    "The loss function is a composite of BCE, and decorrelation from the MET, to ensure the transformer does not sculpt the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc54a0fc-2c67-43ee-a230-a8459938ec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model #\n",
    "loss_function = BCEDecorrelatedLoss(lam = 0.1,weighted=True) # Create new one for multiclass, MDMM mechanism\n",
    "# loss = BCE + lambda * decorrelated loss # WHAT IS LAMBDA\n",
    "# lambda = 0 -> pure BCE\n",
    "\n",
    "# Embedding of the jet features into a higher dimension\n",
    "embedding = Embedding(\n",
    "    input_dim = train_X.shape[-1],   # Input dimension\n",
    "    embed_dims = [64],               # Output dimension\n",
    "    normalize_input = True,          # Whether to apply batch norm before the embedding\n",
    ")\n",
    "\n",
    "model = AnalysisObjectTransformer(\n",
    "    embedding = embedding,           # Embedding instance \n",
    "    embed_dim = embedding.dim,       # Embedding dimension\n",
    "    num_heads = 8,                   # Number of heads for multihead attention (must be a divisor of embed dim) \n",
    "    output_dim = 1,                  # Output dimension (1 : binary classification, >1 : multi classification) \n",
    "    expansion_factor = 4,            # Multipliying factor for layers in attention block (neurons = embed_dim * expansion_factor) norm is 4\n",
    "    encoder_layers = 5,              # Number of encoder layers (self attention on jets)\n",
    "    class_layers = 2,                # Number of class layers (cross attention between jets representations and class token)\n",
    "    dnn_layers = 3,                  # Number of layers for DNN after the transformer\n",
    "    hidden_activation = nn.GELU,     # Hidden activation in transformer and DNN\n",
    "    output_activation = None,        # DNN output activation (sigmoid for binary, softmax for multiclass, None if applied in the loss)\n",
    "    dropout = 0.1,                  # Dropout rate\n",
    "    loss_function = loss_function,   # Loss function, see above\n",
    ")\n",
    "\n",
    "# Print the model graph #\n",
    "print (model)\n",
    "# Quick benchmark test the model #\n",
    "# To make sure the model will run, we take a single batch from the training and pass it through the model\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "inputs, labels, weights, mask, event = batch\n",
    "print ('Batch dtypes',inputs.dtype,labels.dtype,weights.dtype,mask.dtype,event.dtype)\n",
    "\n",
    "outputs = model(inputs,padding_mask=mask)\n",
    "print ('outputs',outputs .shape)\n",
    "loss_values = loss_function(outputs,labels,event,weights)\n",
    "print ('losses',loss_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0ab84f-f29f-45b6-bb53-a617380e1cc6",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Writing the training loop can involve a biut of boilerplate code. To bypass that we use the [pytorch-lightning](https://lightning.ai/docs/pytorch/stable/) package to run the training. \n",
    "\n",
    "The batch steps were defined in the model class, and now we can use the trainer class from lightning to run the training. This takes care of doing the loop, putting data on the GPU if needed, applying the schedule, etc.\n",
    "\n",
    "One way to finetune our training (or monitor it) is through callbacks. Callbacks are class objects that are evaluated at different times during the training, and allow the user to access their model. Below we use a few of those :\n",
    "- model checkpoint : saves the best model parameters at every epoch\n",
    "- early stopping : if the loss values (of the validation) plateau, stops the training (hopefully before overtraining)\n",
    "- plotting : a plotting callback we wrote to plot the classification metrics at every epoch\n",
    "- a few other tweaks\n",
    "\n",
    "Lighting also allows several plotter interface, we use `comet` to log all our metrics (and callback plots) online. You will need an account [here](https://www.comet.com/signup), you can use your github account if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa61d08-a066-4090-8a45-c9c463142c76",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##### Parameters #####\n",
    "epochs = 150\n",
    "# lr = 1e-4\n",
    "lr = 5e-4\n",
    "\n",
    "print(len(train_dataset), len(valid_dataset))\n",
    "\n",
    "steps_per_epoch_train = math.ceil(len(train_dataset)/train_loader.batch_size)\n",
    "steps_per_epoch_valid = math.ceil(len(valid_dataset)/valid_loader.batch_size)\n",
    "\n",
    "print (f'Training   : Batch size = {train_loader.batch_size} => {steps_per_epoch_train} steps per epoch')\n",
    "print (f'Validation : Batch size = {valid_loader.batch_size} => {steps_per_epoch_valid} steps per epoch')\n",
    "\n",
    "# Setup optimizer #\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model.set_optimizer(optimizer)\n",
    "\n",
    "# Setup scheduler #\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "   optimizer = optimizer,\n",
    "   mode = 'min', \n",
    "   factor = 0.25, \n",
    "   patience = 6, #Was 10\n",
    "   threshold = 0., \n",
    "   threshold_mode = 'rel', \n",
    "   cooldown = 0, \n",
    "   min_lr = 1e-8,\n",
    ")\n",
    "model.set_scheduler_config(\n",
    "    {\n",
    "        'scheduler' : scheduler,\n",
    "        'interval' : 'step' if isinstance(scheduler,optim.lr_scheduler.OneCycleLR) else 'epoch',\n",
    "        'frequency' : 1,\n",
    "        'monitor' : 'val/loss_tot',\n",
    "        'strict' : True,\n",
    "        'name' : 'lr',\n",
    "    }\n",
    ")\n",
    "\n",
    "# Setup callbacks #\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=outdir,  # Directory where checkpoints will be saved\n",
    "    filename=\"best-checkpoint\",  # Checkpoint filename\n",
    "    monitor=\"val/loss_tot\",  # Monitor validation loss\n",
    "    mode=\"min\",  # Save the best model with the minimum validation loss\n",
    "    save_top_k=1  # Save only the best model\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val/loss_tot',  # Metric to monitor\n",
    "    patience=12,          # Number of epochs with no improvement after which training will be stopped #Defacult was 30, changed to 8\n",
    "    verbose=True,\n",
    "    mode='min'           # 'min' for loss, 'max' for accuracy\n",
    ")\n",
    "\n",
    "log_bar = L.pytorch.callbacks.TQDMProgressBar(refresh_rate=steps_per_epoch_train//100)\n",
    "\n",
    "plots_callback = EpochEndCallback(\n",
    "    data = valid_loader,\n",
    "    frequency = 1,\n",
    "    subcallbacks = [\n",
    "        ScoreSubCallback(name='score',bins=100,log=True),\n",
    "        CorrelationSubCallback(name='corr',bins=100,log=True),\n",
    "        ROCSubCallback(name='ROC'),\n",
    "        ConfusionMatrixSubCallback(name='CM'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "## Logger ##\n",
    "# logger = CometLogger(\n",
    "#     api_key = os.environ.get(\"COMET_API_KEY\"), \n",
    "#     project_name = \"AnalysisObjectTransformer\",\n",
    "#     experiment_name = \"Setup\",\n",
    "#     save_dir = \"./comet_logs\",  # Specify where to save Comet logs if offline\n",
    "#     offline = False  # Set to True for offline mode\n",
    "\n",
    "\n",
    "## Logger ## (THIS IS THE NEW ONE WITH THE PATH SETUP)\n",
    "logger = CometLogger(\n",
    "    api_key = os.getenv(\"COMET_API_KEY\"),\n",
    "    project_name = \"AnalysisObjectTransformer\",\n",
    "    experiment_name = \"Setup\",\n",
    "    save_dir = \"./comet_logs\",  # Specify where to save Comet logs if offline\n",
    "    offline = False  # Set to True for offline mode\n",
    ")\n",
    "\n",
    "## Trainer ##\n",
    "trainer = L.Trainer(\n",
    "    strategy = \"ddp_notebook\",\n",
    "    default_root_dir = outdir,\n",
    "    accelerator = accelerator,\n",
    "    devices = [0,1,2,3],  # Use only one GPU\n",
    "    max_epochs = epochs,  # Specify the number of epochs\n",
    "    log_every_n_steps = steps_per_epoch_train, # This is logging the wrong number\n",
    "    check_val_every_n_epoch = 1,  # Check validation every n epochs\n",
    "    callbacks = [\n",
    "        checkpoint_callback, \n",
    "        early_stopping,\n",
    "        log_bar,\n",
    "        plots_callback,\n",
    "    ],\n",
    "    logger = logger,\n",
    "    # limit_train_batches = 10,\n",
    "    # limit_val_batches = 1,\n",
    "    # limit_test_batches = 1,\n",
    ")\n",
    "\n",
    "trainer.fit(\n",
    "    model = model, \n",
    "    train_dataloaders = train_loader, \n",
    "    val_dataloaders = valid_loader,\n",
    ")\n",
    "trainer.save_checkpoint(f\"{outdir}/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd8dc8d-5ca2-4389-80c2-2d9537bad100",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing model performance on validation set ##\n",
    "single_gpu_trainer = L.Trainer(accelerator=\"gpu\", devices=1)\n",
    "preds = single_gpu_trainer.predict(model=model, dataloaders=valid_loader)\n",
    "preds = torch.cat(preds, dim=0)\n",
    "\n",
    "inputs, labels, weights, mask, event = valid_dataset.tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c9ba52-ed99-49a3-bd4b-77513e15ff51",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_score(labels,preds,outdir=outdir,bins=100,log=False)\n",
    "fig = plot_score(labels,preds,weights=val_nom,outdir=outdir,bins=100,log=True)\n",
    "fig = plot_confusion_matrix(labels, torch.sigmoid(preds), outdir=outdir,show=True)\n",
    "fig = plot_correlation(labels,preds,event,outdir=outdir,log=True,bins=40)\n",
    "fig = plot_roc(labels, preds, outdir=outdir,show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f5ad5d",
   "metadata": {},
   "source": [
    "## Plot significance + weighted ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88a52ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights array\n",
    "wn = np.array((val_nom * 200000).flatten()) #weight nominal\n",
    "probs = np.array(torch.sigmoid(preds).flatten())\n",
    "labels = np.array(labels.flatten())\n",
    "thresholds = np.linspace(0, 1, 101)\n",
    "z_scores = []\n",
    "\n",
    "print(wn.shape, probs.shape, labels.shape)\n",
    "\n",
    "# Calculate the Z-score for each threshold using weights\n",
    "for threshold in thresholds:\n",
    "    # Weighted count of signal predictions above the threshold\n",
    "    S = np.sum(wn[(probs > threshold) & (labels == 1)])  # Weighted Signal\n",
    "    B = np.sum(wn[(probs > threshold) & (labels == 0)])  # Weighted Background\n",
    "    sig_B = B * 0.05 # 5% error\n",
    "    \n",
    "    Z = S / np.sqrt(B + sig_B**2 + 1e-10)\n",
    "\n",
    "    z_scores.append(Z)\n",
    "\n",
    "# Plot Z-score vs. Threshold\n",
    "plt.plot(thresholds, z_scores, label=\"Z-score\")\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Z-score\")\n",
    "plt.title(f\"Z-score vs. Threshold, Max Z-score: {max(z_scores[:95]):.4f}\")\n",
    "plt.grid()\n",
    "plt.ylim(0,10)\n",
    "plt.xlim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b86a0d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weighted_roc_curve(y_true, y_pred_proba, sample_weights=None, title=\"Weighted ROC Curve\"):\n",
    "    \"\"\"\n",
    "    Plot a ROC curve with optional sample weights.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True binary labels\n",
    "    y_pred_proba : array-like\n",
    "        Predicted probabilities for the positive class\n",
    "    sample_weights : array-like, optional\n",
    "        Sample weights for each observation\n",
    "    title : str, optional\n",
    "        Title for the plot\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    tuple\n",
    "        (fpr, tpr, thresholds, weighted_auc)\n",
    "    \"\"\"\n",
    "    # If no weights provided, use uniform weights\n",
    "    if sample_weights is None:\n",
    "        sample_weights = np.ones(len(y_true))\n",
    "    \n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba, sample_weight=sample_weights)\n",
    "    \n",
    "    # Calculate weighted AUC\n",
    "    # We'll use the trapezoidal rule to compute the area\n",
    "    weighted_auc = np.trapz(tpr, fpr)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(fpr, tpr, 'b-', label=f'Weighted ROC (AUC = {weighted_auc:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], 'r--', label='Random Classifier')\n",
    "    \n",
    "    # Customize the plot\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Set the plot limits\n",
    "    plt.xlim([-0.02, 1.02])\n",
    "    plt.ylim([-0.02, 1.02])\n",
    "\n",
    "    \n",
    "    return fpr, tpr, thresholds, weighted_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2ef3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weighted_roc_curve(labels, probs, wn, title=\"Weighted ROC Curve\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c041f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histogram of the predictions\n",
    "\n",
    "plt.hist(probs[labels == 0], bins=100, range=(0, 1), weights=wn[labels == 0], histtype='step', label='Background', density = True)\n",
    "plt.hist(probs[labels == 1], bins=100, range=(0, 1), weights=wn[labels == 1], histtype='step', label='Signal', density = True)\n",
    "plt.legend()  \n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('Events')\n",
    "plt.title('Prediction distribution (normalised)')   \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a8e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hinv-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
