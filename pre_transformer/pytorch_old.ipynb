{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# import dataprep # this not working\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "    \n",
    "# Import the data\n",
    "signal_data = pd.read_hdf(\"/dice/projects/CMS/Hinv/datasets_for_ml_training/ttH/df_ml_inputs.hd5\")\n",
    "background_data = pd.read_hdf(\"/dice/projects/CMS/Hinv/datasets_for_ml_training/ttBar/df_ml_inputs.hd5\")\n",
    "\n",
    "def prep(signal_data, background_data):\n",
    "\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "\n",
    "    # Remove nan values\n",
    "    signal_data = signal_data.dropna()\n",
    "    background_data = background_data.dropna()\n",
    "\n",
    "    # Remove regions 1-5 as they contain data not suitable for training\n",
    "    signal_data = signal_data[~signal_data.region.isin([1, 2, 3, 4, 5])]\n",
    "    background_data = background_data[~background_data.region.isin([1, 2, 3, 4, 5])]\n",
    "\n",
    "    # Adding in balance weights \n",
    "\n",
    "    signal_data['weight_balance'] = len(background_data) / len(signal_data)\n",
    "    background_data['weight_balance'] = 1\n",
    "\n",
    "    # Dealing with class imbalance by reducing the number of background events\n",
    "    # total_background_weight = background_data['weight_nominal'].sum() # Step 1: Calculate total weight for the original background\n",
    "    # sampled_background = background_data.sample(len(signal_data), random_state=42) # Step 2: Randomly sample x rows from the background data\n",
    "    # sampled_background_weight = sampled_background['weight_nominal'].sum() # Step 3: Calculate the total weight of the sampled background (before scaling)\n",
    "    # scaling_factor = total_background_weight / sampled_background_weight # Step 4: Compute the scaling factor to adjust the weights\n",
    "    # sampled_background['weight_nominal_scaled'] = sampled_background['weight_nominal'] * scaling_factor # Step 5: Scale the weights of the sampled background rows\n",
    "    # signal_data['weight_nominal_scaled'] = signal_data['weight_nominal'] # Add scaled weight ot the signal data\n",
    "\n",
    "    # Combine data + add feautures\n",
    "    # signal_data['target'] = 1\n",
    "    # sampled_background['target'] = 0\n",
    "    # data = pd.concat([signal_data, sampled_background])\n",
    "\n",
    "    # Combine data + add feautures\n",
    "    signal_data['target'] = 1\n",
    "    background_data['target'] = 0\n",
    "    data = pd.concat([signal_data, background_data])\n",
    "\n",
    "    # Creation of additional useful features found from 'investigatingjets.ipynb'\n",
    "    # cleanedJet_eta_std, cleanedJet_eta_range, cleadJet_phi_std, cleanedJet_phi_range\n",
    "\n",
    "    data['cleanedJet_eta_std'] = data['cleanedJet_eta'].apply(lambda x: np.std(x))\n",
    "    data['cleanedJet_eta_range'] = data['cleanedJet_eta'].apply(lambda x: np.max(x) - np.min(x))\n",
    "    data['cleanedJet_phi_std'] = data['cleanedJet_phi'].apply(lambda x: np.std(x))\n",
    "    data['cleanedJet_phi_range'] = data['cleanedJet_phi'].apply(lambda x: np.max(x) - np.min(x))\n",
    "\n",
    "    return data\n",
    "\n",
    "data = prep(signal_data, background_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the old code for when we dealt with class imbalance by removing samples\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(data[features].values, dtype=torch.float32)\n",
    "y = torch.tensor(data['target'].values, dtype=torch.float32)  # For BCE loss, use float labels\n",
    "weights_nominal = torch.tensor(data['weight_nominal_scaled'].values, dtype=torch.float32) # Use this one for saving the weights when balancing by removing\n",
    "\n",
    "# Split data into training+validation and test sets (80/20)\n",
    "X_train_val, X_test, y_train_val, y_test, wn_train_val, wn_test = train_test_split(X, y, weights_nominal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into train and validation sets (80/20 of 80%)\n",
    "X_train, X_val, y_train, y_val, wn_train, wn_val  = train_test_split(X_train_val, y_train_val, wn_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train, wn_train), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val, wn_val), batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test, wn_test), batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "n_features = len(features)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_layers=1, n_neurons=64):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(n_features, n_neurons))\n",
    "        layers.append(nn.BatchNorm1d(n_neurons))  # Add BatchNorm after the input layer\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(n_neurons, n_neurons))\n",
    "            layers.append(nn.BatchNorm1d(n_neurons))  # Add BatchNorm after each hidden layer\n",
    "            \n",
    "        layers.append(nn.Linear(n_neurons, 1)) # Final layer with 1 neuron for binary classification\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.sigmoid(self.layers[-1](x)) #Temp moved\n",
    "        # x = self.layers[-1](x) # USE THIS LINE FOR BCEWithLogitsLoss\n",
    "        return x\n",
    "\n",
    "model = SimpleNN(n_layers=2, n_neurons=64) # This is where we define the model\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss(reduction=\"mean\")  # Binary cross entropy \n",
    "# criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")  # Binary cross entropy with logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model with validation tracking\n",
    "n_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for batch_x, batch_y, batch_wn in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        batch_y = batch_y.view(-1, 1)  # Reshape to match output shape\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_wn in val_loader:\n",
    "            outputs = model(batch_x)\n",
    "            batch_y = batch_y.view(-1, 1)\n",
    "            val_loss = criterion(outputs, batch_y)\n",
    "            running_val_loss += val_loss.item() * batch_x.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.plot(range(n_epochs), train_losses, label='Training Loss')\n",
    "plt.plot(range(n_epochs), val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(range(n_epochs))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
