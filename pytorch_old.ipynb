{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the old code for when we dealt with class imbalance by removing samples\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "data[features] = scaler.fit_transform(data[features])\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X = torch.tensor(data[features].values, dtype=torch.float32)\n",
    "y = torch.tensor(data['target'].values, dtype=torch.float32)  # For BCE loss, use float labels\n",
    "weights_nominal = torch.tensor(data['weight_nominal_scaled'].values, dtype=torch.float32) # Use this one for saving the weights when balancing by removing\n",
    "\n",
    "# Split data into training+validation and test sets (80/20)\n",
    "X_train_val, X_test, y_train_val, y_test, wn_train_val, wn_test = train_test_split(X, y, weights_nominal, test_size=0.2, random_state=42)\n",
    "\n",
    "# Further split the training data into train and validation sets (80/20 of 80%)\n",
    "X_train, X_val, y_train, y_val, wn_train, wn_val  = train_test_split(X_train_val, y_train_val, wn_train_val, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create DataLoaders for each set\n",
    "train_loader = DataLoader(TensorDataset(X_train, y_train, wn_train), batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val, y_val, wn_val), batch_size=64, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test, y_test, wn_test), batch_size=64, shuffle=False)\n",
    "\n",
    "# Define the model\n",
    "n_features = len(features)\n",
    "\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, n_layers=1, n_neurons=64):\n",
    "        super(SimpleNN, self).__init__()\n",
    "\n",
    "        layers = []\n",
    "        layers.append(nn.Linear(n_features, n_neurons))\n",
    "        layers.append(nn.BatchNorm1d(n_neurons))  # Add BatchNorm after the input layer\n",
    "\n",
    "        for _ in range(n_layers - 1):\n",
    "            layers.append(nn.Linear(n_neurons, n_neurons))\n",
    "            layers.append(nn.BatchNorm1d(n_neurons))  # Add BatchNorm after each hidden layer\n",
    "            \n",
    "        layers.append(nn.Linear(n_neurons, 1)) # Final layer with 1 neuron for binary classification\n",
    "\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.sigmoid = nn.Sigmoid() \n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = torch.relu(layer(x))\n",
    "        x = self.sigmoid(self.layers[-1](x)) #Temp moved\n",
    "        # x = self.layers[-1](x) # USE THIS LINE FOR BCEWithLogitsLoss\n",
    "        return x\n",
    "\n",
    "model = SimpleNN(n_layers=2, n_neurons=64) # This is where we define the model\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.BCELoss(reduction=\"mean\")  # Binary cross entropy \n",
    "# criterion = nn.BCEWithLogitsLoss(reduction=\"mean\")  # Binary cross entropy with logits\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model with validation tracking\n",
    "n_epochs = 20\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_train_loss = 0.0\n",
    "    for batch_x, batch_y, batch_wn in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        batch_y = batch_y.view(-1, 1)  # Reshape to match output shape\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_train_loss += loss.item() * batch_x.size(0)\n",
    "        \n",
    "    epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
    "    train_losses.append(epoch_train_loss)\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    running_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y, batch_wn in val_loader:\n",
    "            outputs = model(batch_x)\n",
    "            batch_y = batch_y.view(-1, 1)\n",
    "            val_loss = criterion(outputs, batch_y)\n",
    "            running_val_loss += val_loss.item() * batch_x.size(0)\n",
    "\n",
    "    epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
    "    val_losses.append(epoch_val_loss)\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{n_epochs}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_val_loss:.4f}')\n",
    "\n",
    "# Plot the training and validation losses\n",
    "plt.plot(range(n_epochs), train_losses, label='Training Loss')\n",
    "plt.plot(range(n_epochs), val_losses, label='Validation Loss')\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.xticks(range(n_epochs))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
